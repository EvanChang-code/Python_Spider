## TypeError
* 'NoneType' object is not callable *模型信息不可调用*

## 租房子信息分析
**要求：对网站进行分析，解析出其网页链接的规律，然后爬取主题，地址，租金，图片链接，主人姓名，性别等信息**

* 网页解析后是可迭代的对象，不能使用`get()`方法
* 迭代后得到的结果是`list`，所以在对性别进行判断的时候，是`['member_ico']`而不是`'member_ico'`的
* `{} .format()`是处理字符串的一种形式
* 元素的形式可以使混合的`'div[class="number"] > li > he.class'`

## 爬取一页商品的数据
**要求：基本问题同租房子信息分析，另：浏览量是动态加载出来，需要进行js分析**

* `#`表示`id`，`.`表示`class`
* 对数据进行对位的时候，可以不适用`>`，直接对元素进行跨子标签定位，或以` `(空格)来代替`>`
* `select()`返回的是一个`list`，所以在分析数据的时候，可以不适用`for`来迭代，直接用坐标的方式进行就好
* 使用字符分割函数，和`format`
* 可以使用`None`来进行占位符
* 字典解析式

例：num为0返回yes，否则返回no

	'yes' if num==0 else 'no'

## 爬取的数据写入到数据库中
* 对数据库的操作
* 需要注意的问题：被爬取网站的页码限制

## 爬取58同城所有的二手商品的数据
* 对于很多的字符串用''' '''表示
* 对于`split()`函数的灵活使用
* 数据库中需要插入的字典或者是其他
* 对于进程池的使用

* 快速检索出需要的信息
* 在没有的时候，怎么停止，时写return或者是跳出
* IP的获取，http://cn-proxy.com,使用ranodm.choice()方法
	

**这周的问题**


* 1. 在第一天的作业中没有解析出一个细节的问题
* 2. 在爬取照片的时候，遇见了强制关闭的状态，可能是写错某一个地方了，header的地方可能是有问题的
* 网页请求的过程
* request 时 get和post 基本内容html 、协议HTTP 、Host
请求网页 然后有requose
解析网页 
整理并筛选所需要的信息   标签[xxx]  cate.strippend_string

确定元素：某种元素有某种属性，或者某种元素的唯一性
多次判断元素是否合适

向服务器提交的参数
get(url, header)

 * 要全部的页面进行的爬虫信息
首先页面的规律是怎么搞
1. 列表的解析式
		频率限制 time.sleep()
2. 每页的请求信息

如果js不能打开的话，可以伪装手机进行请求

* 房屋招聘信息的问题：

思路：显示的问题，可以写一个正则表达式匹配一下
然后，图片是不是可以进入网页的链接

简单的思路就是对网页进行请求，不停的请求，然后解析出页面来

* 爬取网站的异步加载数据
什么是异步加载：不换页码也可以进行不停的加载
如何发现异步数据，Network XHR
然后是装载数据，最后进行的是怎么样可以将数据完整的提取出来


* MongoDB数据的存储
激活本地的链接（localhost本地环境）pymongo.MongoClient
然后给数据库起一个名称client
然后在一个文件下创建一个表单，方式按照对象和数据表的样式。
打开本地的文本然后，读取文件的名称，然后构建一个字典，每一行的文字，每一行在文件的哪一行，然后这行包含的文字的数量
然后往数据库中写入数据=在excal中写入每一行的数据
怎么查看数据是否成功了，可以读出来，.find() 可视化
找数据的话，然后find方法，找到具体的哪一个后面加一个大括号

* 爬取大规模的数据
实际上没有这么多的页面，然后需要规避
设计工作流程
从列表页找到url，然后存取在mongodb的数据库中
然后抓取详情页，spide2 从mogodb中取出url，然后得到详情页的数据
code——Start
1. 找到所有的频道链接  怎么凑出一个完整的链接，简化为字符串的操作，字符串直接加就行
 非常长串的用''''''
2. 然后将频道页下的每页的网址找到，（频道页按照{}{}的方式写），然后抓取频道页下的链接，放入到数据库中，学习数据库的操作方式
	注意可能页面中没有链接
3. 抓取一个链接中的信息，放入到数据库中
	注意存在404不能访问的页面

## 多线程和多进程
* 创建一个新的py文件
* 多进程的库，multiprocessing import Pool
* 引用自己的库
* 从文件中导入方法
* 构建一个新的函数

* 创建进程池 pool  = Pool(),所有的爬虫都会被扔进池子中，自动分配
* map函数

## 让数据说话

	tab会提醒
	enter是下一行
	shift+enter是执行一个cell中的代码
* 整理清洗数据
	链接到数据库
	将数据整理出来
	引入标点符号库 from string import punctuation
	
		使用索引的方法进行,{'_id':i['id']} # 因为都会存在`_id`
* 更新到数据库
	一般修改数据前，需要先备份数据，新建terminal，
		mongod
		mongo
		show dbs
		use xx
		show tables (collections)
		db.createCollection('xx')创建新的数据库
		db.xx.copyTo('xx')
	然后重新放入数据库中，使用update(file, change where) 
		file:{id:1}(元素)
		change where {$set:{name:2}}表示怎么修改
		将{id:1}改变成{name:2} 
* 数据的可视化

## 数据的变化曲线
* find(file, 返回的格式)
	file为条件操作，$in
	返回的格式，即是在被删选出的文件进行在再次的筛选 $slice
	也可以直接的使用第二个参数，忽略第一个参数
* 使用列表解析式来将`·`变成`-`
* 引入新的模块
步骤是怎么样的呢？
1. 日期，地区就是取北京的，日期的格式需要改正一下
2. 熟悉日期这个模块
3. 关于日期生成每日的函数
4. 在生成地区的函数，这里面涉及的问题就是怎么和时间对应上的

## 某一天二手商品在随后的七天内，随后七天内的类目分布占比
* 某一个技术型的公司，在随后的七天内，时长为一天的融资个数或者是融资的钱数分布占比
* collection.aggregate(pipeline)
通过管道得到数据个数，不同的管道有不同的作用， 有的管道进行筛选，有的管道进行重命名，有的可以聚合
	pipeline = {
		{'$match':{'pub_date':''}}
		多条件运算
		{'$match':{'$and':[{},{},{}]}},

		{'$group':{'_id': 'counts':{'$sum':1}}} 接收两个参数，_id  以什么来分组，后边执行的什么 1表示+1，可以是2，是3

		{'$sort':{'counts':1}}  sort表示按照一定的排序，1表示正序 

		{'limit':3}  限制数量
	}











